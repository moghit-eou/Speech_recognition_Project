{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3eec434a-1005-4e11-95ea-81d5294e2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "SAMPLES_TO_CONSIDER = 22050 # \n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"MFCCs\": [],\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save label like (one , hello , up , down ... ) in the mapping\n",
    "            label = dirpath.split(\"/\")[-1]\n",
    "            data[\"mapping\"].append(label)\n",
    "\n",
    "            if ( label[-1] == '_' ) : # for skipping the background music \n",
    "                continue\n",
    "            print(\"\\nProcessing: '{}'\".format(label))\n",
    "\n",
    "            counter = 0\n",
    "            #checking the sub-folders as well \n",
    "            for f in filenames: \n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                counter = counter + 1\n",
    "\n",
    "                #no need to iterate the entire the data set just pick 30 from each for testing model only\n",
    "                #  after we will the whole dataset\n",
    "                if ( counter == 30 ):  \t\n",
    "                    break\n",
    "\n",
    "                signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "                if len(signal) >= SAMPLES_TO_CONSIDER:\n",
    "\n",
    "                    signal = signal[:SAMPLES_TO_CONSIDER]\n",
    "\n",
    "                    # extract MFCCs from the audio\n",
    "                    MFCCs = librosa.feature.mfcc(y=signal, sr = sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                                 hop_length=hop_length)\n",
    "\n",
    "                    data[\"MFCCs\"].append(MFCCs.T.tolist())\n",
    "                    data[\"labels\"].append(i-1)\n",
    "                    data[\"files\"].append(file_path)\n",
    "\n",
    "\n",
    "    print(\"loading into json file\")\n",
    "    with open(json_path , \"w\") as fp:\n",
    "        json.dump(data , fp , indent = 4 ) \n",
    "    print(\"data is loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05e110c0-e331-4296-825f-f31e1f25f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'real_dataset\\bed'\n",
      "\n",
      "Processing: 'real_dataset\\bird'\n",
      "\n",
      "Processing: 'real_dataset\\cat'\n",
      "\n",
      "Processing: 'real_dataset\\dog'\n",
      "\n",
      "Processing: 'real_dataset\\down'\n",
      "\n",
      "Processing: 'real_dataset\\eight'\n",
      "\n",
      "Processing: 'real_dataset\\five'\n",
      "\n",
      "Processing: 'real_dataset\\four'\n",
      "\n",
      "Processing: 'real_dataset\\go'\n",
      "\n",
      "Processing: 'real_dataset\\happy'\n",
      "\n",
      "Processing: 'real_dataset\\house'\n",
      "\n",
      "Processing: 'real_dataset\\left'\n",
      "\n",
      "Processing: 'real_dataset\\marvin'\n",
      "\n",
      "Processing: 'real_dataset\\nine'\n",
      "\n",
      "Processing: 'real_dataset\\no'\n",
      "\n",
      "Processing: 'real_dataset\\off'\n",
      "\n",
      "Processing: 'real_dataset\\on'\n",
      "\n",
      "Processing: 'real_dataset\\one'\n",
      "\n",
      "Processing: 'real_dataset\\right'\n",
      "\n",
      "Processing: 'real_dataset\\seven'\n",
      "\n",
      "Processing: 'real_dataset\\sheila'\n",
      "\n",
      "Processing: 'real_dataset\\six'\n",
      "\n",
      "Processing: 'real_dataset\\stop'\n",
      "\n",
      "Processing: 'real_dataset\\three'\n",
      "\n",
      "Processing: 'real_dataset\\tree'\n",
      "\n",
      "Processing: 'real_dataset\\two'\n",
      "\n",
      "Processing: 'real_dataset\\up'\n",
      "\n",
      "Processing: 'real_dataset\\wow'\n",
      "\n",
      "Processing: 'real_dataset\\yes'\n",
      "\n",
      "Processing: 'real_dataset\\zero'\n",
      "loading into json file\n",
      "data is loaded\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"source/real_dataset\"\n",
    "JSON_PATH = \"input_data.json\"\n",
    "preprocess_dataset(DATASET_PATH , JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "676ecdde-39b2-49b6-9b1a-3e9b5dcc5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"MFCCs\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    print(\"Training sets loaded , hope they work\")\n",
    "    return X, y\n",
    "input_data = \"input_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e3069aff-634a-4598-a5bf-06243e3e1e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sets loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-525.07</td>\n",
       "      <td>76.78</td>\n",
       "      <td>-11.60</td>\n",
       "      <td>52.65</td>\n",
       "      <td>-11.82</td>\n",
       "      <td>23.17</td>\n",
       "      <td>3.46</td>\n",
       "      <td>9.32</td>\n",
       "      <td>4.81</td>\n",
       "      <td>9.84</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0.30</td>\n",
       "      <td>11.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-515.70</td>\n",
       "      <td>58.14</td>\n",
       "      <td>-28.15</td>\n",
       "      <td>60.27</td>\n",
       "      <td>-13.81</td>\n",
       "      <td>24.47</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>9.37</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.76</td>\n",
       "      <td>10.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-512.24</td>\n",
       "      <td>43.76</td>\n",
       "      <td>-42.25</td>\n",
       "      <td>60.95</td>\n",
       "      <td>-16.35</td>\n",
       "      <td>29.58</td>\n",
       "      <td>0.37</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.67</td>\n",
       "      <td>2.92</td>\n",
       "      <td>14.63</td>\n",
       "      <td>6.03</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-506.90</td>\n",
       "      <td>46.93</td>\n",
       "      <td>-43.91</td>\n",
       "      <td>60.30</td>\n",
       "      <td>-17.97</td>\n",
       "      <td>27.71</td>\n",
       "      <td>2.81</td>\n",
       "      <td>9.62</td>\n",
       "      <td>8.36</td>\n",
       "      <td>7.00</td>\n",
       "      <td>16.61</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-515.04</td>\n",
       "      <td>57.47</td>\n",
       "      <td>-34.75</td>\n",
       "      <td>58.05</td>\n",
       "      <td>-13.05</td>\n",
       "      <td>23.33</td>\n",
       "      <td>4.30</td>\n",
       "      <td>14.76</td>\n",
       "      <td>9.27</td>\n",
       "      <td>6.74</td>\n",
       "      <td>11.68</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>11.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-518.08</td>\n",
       "      <td>59.62</td>\n",
       "      <td>-28.44</td>\n",
       "      <td>62.34</td>\n",
       "      <td>-5.90</td>\n",
       "      <td>28.66</td>\n",
       "      <td>4.44</td>\n",
       "      <td>12.78</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.10</td>\n",
       "      <td>7.76</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-519.60</td>\n",
       "      <td>59.02</td>\n",
       "      <td>-29.47</td>\n",
       "      <td>58.88</td>\n",
       "      <td>-7.06</td>\n",
       "      <td>28.62</td>\n",
       "      <td>5.67</td>\n",
       "      <td>12.70</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.17</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-521.95</td>\n",
       "      <td>51.18</td>\n",
       "      <td>-39.10</td>\n",
       "      <td>50.69</td>\n",
       "      <td>-14.17</td>\n",
       "      <td>28.43</td>\n",
       "      <td>8.27</td>\n",
       "      <td>12.75</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.46</td>\n",
       "      <td>6.95</td>\n",
       "      <td>4.85</td>\n",
       "      <td>12.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-520.59</td>\n",
       "      <td>49.49</td>\n",
       "      <td>-43.04</td>\n",
       "      <td>47.08</td>\n",
       "      <td>-17.83</td>\n",
       "      <td>26.43</td>\n",
       "      <td>6.32</td>\n",
       "      <td>13.32</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.81</td>\n",
       "      <td>12.11</td>\n",
       "      <td>7.06</td>\n",
       "      <td>14.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-525.05</td>\n",
       "      <td>50.01</td>\n",
       "      <td>-40.06</td>\n",
       "      <td>49.56</td>\n",
       "      <td>-16.72</td>\n",
       "      <td>22.40</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.81</td>\n",
       "      <td>3.87</td>\n",
       "      <td>7.40</td>\n",
       "      <td>9.82</td>\n",
       "      <td>0.11</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-526.81</td>\n",
       "      <td>46.46</td>\n",
       "      <td>-38.17</td>\n",
       "      <td>55.42</td>\n",
       "      <td>-12.63</td>\n",
       "      <td>22.49</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>6.59</td>\n",
       "      <td>5.84</td>\n",
       "      <td>7.95</td>\n",
       "      <td>8.60</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>13.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-523.35</td>\n",
       "      <td>53.00</td>\n",
       "      <td>-38.75</td>\n",
       "      <td>51.62</td>\n",
       "      <td>-10.86</td>\n",
       "      <td>23.17</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>13.58</td>\n",
       "      <td>11.15</td>\n",
       "      <td>7.36</td>\n",
       "      <td>9.68</td>\n",
       "      <td>2.17</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-521.51</td>\n",
       "      <td>57.29</td>\n",
       "      <td>-33.59</td>\n",
       "      <td>55.48</td>\n",
       "      <td>-7.68</td>\n",
       "      <td>28.77</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14.94</td>\n",
       "      <td>13.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>10.51</td>\n",
       "      <td>1.43</td>\n",
       "      <td>10.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-523.54</td>\n",
       "      <td>55.39</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>59.78</td>\n",
       "      <td>-6.13</td>\n",
       "      <td>30.86</td>\n",
       "      <td>4.96</td>\n",
       "      <td>10.88</td>\n",
       "      <td>9.95</td>\n",
       "      <td>10.27</td>\n",
       "      <td>6.78</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>13.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-525.72</td>\n",
       "      <td>51.37</td>\n",
       "      <td>-31.96</td>\n",
       "      <td>56.56</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>33.38</td>\n",
       "      <td>5.11</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.55</td>\n",
       "      <td>8.82</td>\n",
       "      <td>5.45</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>13.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-526.79</td>\n",
       "      <td>54.23</td>\n",
       "      <td>-27.46</td>\n",
       "      <td>57.25</td>\n",
       "      <td>-13.31</td>\n",
       "      <td>30.25</td>\n",
       "      <td>7.24</td>\n",
       "      <td>11.57</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.64</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-526.16</td>\n",
       "      <td>52.92</td>\n",
       "      <td>-37.38</td>\n",
       "      <td>50.96</td>\n",
       "      <td>-9.34</td>\n",
       "      <td>27.12</td>\n",
       "      <td>4.61</td>\n",
       "      <td>15.23</td>\n",
       "      <td>7.53</td>\n",
       "      <td>5.47</td>\n",
       "      <td>8.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-520.05</td>\n",
       "      <td>50.09</td>\n",
       "      <td>-40.47</td>\n",
       "      <td>56.38</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>33.56</td>\n",
       "      <td>4.90</td>\n",
       "      <td>9.40</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.39</td>\n",
       "      <td>6.59</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>12.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-514.36</td>\n",
       "      <td>59.53</td>\n",
       "      <td>-29.91</td>\n",
       "      <td>58.81</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>34.88</td>\n",
       "      <td>4.92</td>\n",
       "      <td>8.42</td>\n",
       "      <td>5.23</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.29</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>6.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-514.69</td>\n",
       "      <td>64.56</td>\n",
       "      <td>-22.60</td>\n",
       "      <td>61.95</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>36.47</td>\n",
       "      <td>6.32</td>\n",
       "      <td>8.86</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.74</td>\n",
       "      <td>5.61</td>\n",
       "      <td>-7.88</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-517.48</td>\n",
       "      <td>62.91</td>\n",
       "      <td>-21.74</td>\n",
       "      <td>66.36</td>\n",
       "      <td>-5.58</td>\n",
       "      <td>32.10</td>\n",
       "      <td>4.74</td>\n",
       "      <td>7.53</td>\n",
       "      <td>7.89</td>\n",
       "      <td>8.64</td>\n",
       "      <td>7.28</td>\n",
       "      <td>-7.72</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-517.11</td>\n",
       "      <td>63.21</td>\n",
       "      <td>-22.48</td>\n",
       "      <td>67.24</td>\n",
       "      <td>-4.92</td>\n",
       "      <td>28.35</td>\n",
       "      <td>2.62</td>\n",
       "      <td>9.92</td>\n",
       "      <td>11.52</td>\n",
       "      <td>10.97</td>\n",
       "      <td>10.49</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-520.01</td>\n",
       "      <td>57.55</td>\n",
       "      <td>-29.15</td>\n",
       "      <td>64.23</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>28.22</td>\n",
       "      <td>3.83</td>\n",
       "      <td>11.41</td>\n",
       "      <td>12.17</td>\n",
       "      <td>8.56</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.66</td>\n",
       "      <td>8.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-445.34</td>\n",
       "      <td>76.91</td>\n",
       "      <td>-63.10</td>\n",
       "      <td>66.46</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>8.79</td>\n",
       "      <td>11.68</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-291.95</td>\n",
       "      <td>84.97</td>\n",
       "      <td>-123.77</td>\n",
       "      <td>95.94</td>\n",
       "      <td>-50.79</td>\n",
       "      <td>-9.73</td>\n",
       "      <td>-21.06</td>\n",
       "      <td>-21.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12.16</td>\n",
       "      <td>13.14</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-242.78</td>\n",
       "      <td>67.01</td>\n",
       "      <td>-148.01</td>\n",
       "      <td>110.22</td>\n",
       "      <td>-63.84</td>\n",
       "      <td>-15.82</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-18.67</td>\n",
       "      <td>3.44</td>\n",
       "      <td>12.76</td>\n",
       "      <td>11.51</td>\n",
       "      <td>7.32</td>\n",
       "      <td>16.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-241.73</td>\n",
       "      <td>44.92</td>\n",
       "      <td>-162.19</td>\n",
       "      <td>121.68</td>\n",
       "      <td>-62.99</td>\n",
       "      <td>-20.57</td>\n",
       "      <td>-30.21</td>\n",
       "      <td>-6.43</td>\n",
       "      <td>8.98</td>\n",
       "      <td>10.25</td>\n",
       "      <td>16.96</td>\n",
       "      <td>15.87</td>\n",
       "      <td>21.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-250.38</td>\n",
       "      <td>44.75</td>\n",
       "      <td>-159.20</td>\n",
       "      <td>134.61</td>\n",
       "      <td>-53.56</td>\n",
       "      <td>-20.17</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.21</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21.04</td>\n",
       "      <td>16.70</td>\n",
       "      <td>20.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-265.88</td>\n",
       "      <td>42.97</td>\n",
       "      <td>-148.57</td>\n",
       "      <td>142.45</td>\n",
       "      <td>-53.35</td>\n",
       "      <td>-24.04</td>\n",
       "      <td>-13.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>22.22</td>\n",
       "      <td>13.72</td>\n",
       "      <td>14.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-280.61</td>\n",
       "      <td>36.83</td>\n",
       "      <td>-139.14</td>\n",
       "      <td>147.86</td>\n",
       "      <td>-56.06</td>\n",
       "      <td>-24.66</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>3.17</td>\n",
       "      <td>-15.92</td>\n",
       "      <td>-6.97</td>\n",
       "      <td>22.45</td>\n",
       "      <td>4.42</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-300.41</td>\n",
       "      <td>25.31</td>\n",
       "      <td>-126.80</td>\n",
       "      <td>154.64</td>\n",
       "      <td>-52.92</td>\n",
       "      <td>-16.80</td>\n",
       "      <td>2.96</td>\n",
       "      <td>9.93</td>\n",
       "      <td>-20.84</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>21.79</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-313.69</td>\n",
       "      <td>21.77</td>\n",
       "      <td>-119.17</td>\n",
       "      <td>152.06</td>\n",
       "      <td>-53.26</td>\n",
       "      <td>-17.89</td>\n",
       "      <td>0.61</td>\n",
       "      <td>12.41</td>\n",
       "      <td>-25.33</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>20.35</td>\n",
       "      <td>-9.30</td>\n",
       "      <td>12.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-306.70</td>\n",
       "      <td>34.26</td>\n",
       "      <td>-121.97</td>\n",
       "      <td>140.33</td>\n",
       "      <td>-48.99</td>\n",
       "      <td>-20.28</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>11.23</td>\n",
       "      <td>-29.19</td>\n",
       "      <td>-3.38</td>\n",
       "      <td>15.13</td>\n",
       "      <td>-6.05</td>\n",
       "      <td>19.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-290.12</td>\n",
       "      <td>57.77</td>\n",
       "      <td>-128.74</td>\n",
       "      <td>123.46</td>\n",
       "      <td>-46.93</td>\n",
       "      <td>-22.48</td>\n",
       "      <td>-10.79</td>\n",
       "      <td>10.41</td>\n",
       "      <td>-19.91</td>\n",
       "      <td>0.62</td>\n",
       "      <td>14.35</td>\n",
       "      <td>2.71</td>\n",
       "      <td>27.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-289.95</td>\n",
       "      <td>74.63</td>\n",
       "      <td>-130.34</td>\n",
       "      <td>105.17</td>\n",
       "      <td>-49.98</td>\n",
       "      <td>-23.01</td>\n",
       "      <td>-14.19</td>\n",
       "      <td>7.82</td>\n",
       "      <td>-7.19</td>\n",
       "      <td>4.14</td>\n",
       "      <td>13.95</td>\n",
       "      <td>5.42</td>\n",
       "      <td>34.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-312.65</td>\n",
       "      <td>65.17</td>\n",
       "      <td>-127.94</td>\n",
       "      <td>95.80</td>\n",
       "      <td>-50.55</td>\n",
       "      <td>-14.60</td>\n",
       "      <td>-15.87</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>10.61</td>\n",
       "      <td>15.95</td>\n",
       "      <td>1.14</td>\n",
       "      <td>37.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-344.59</td>\n",
       "      <td>61.19</td>\n",
       "      <td>-116.77</td>\n",
       "      <td>93.32</td>\n",
       "      <td>-44.63</td>\n",
       "      <td>-10.40</td>\n",
       "      <td>-12.44</td>\n",
       "      <td>6.72</td>\n",
       "      <td>8.73</td>\n",
       "      <td>19.25</td>\n",
       "      <td>18.55</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>33.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-369.96</td>\n",
       "      <td>70.56</td>\n",
       "      <td>-106.67</td>\n",
       "      <td>84.97</td>\n",
       "      <td>-36.27</td>\n",
       "      <td>-13.31</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>3.05</td>\n",
       "      <td>7.95</td>\n",
       "      <td>22.23</td>\n",
       "      <td>15.13</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>32.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-390.19</td>\n",
       "      <td>71.79</td>\n",
       "      <td>-97.79</td>\n",
       "      <td>81.49</td>\n",
       "      <td>-30.05</td>\n",
       "      <td>-12.01</td>\n",
       "      <td>-10.34</td>\n",
       "      <td>1.56</td>\n",
       "      <td>6.70</td>\n",
       "      <td>22.31</td>\n",
       "      <td>11.74</td>\n",
       "      <td>-4.74</td>\n",
       "      <td>32.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-394.03</td>\n",
       "      <td>67.42</td>\n",
       "      <td>-87.92</td>\n",
       "      <td>91.77</td>\n",
       "      <td>-29.34</td>\n",
       "      <td>-6.93</td>\n",
       "      <td>-9.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.69</td>\n",
       "      <td>11.67</td>\n",
       "      <td>-12.71</td>\n",
       "      <td>33.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-425.23</td>\n",
       "      <td>64.37</td>\n",
       "      <td>-69.29</td>\n",
       "      <td>91.85</td>\n",
       "      <td>-25.13</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>14.87</td>\n",
       "      <td>8.40</td>\n",
       "      <td>-20.55</td>\n",
       "      <td>27.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-492.60</td>\n",
       "      <td>59.90</td>\n",
       "      <td>-40.21</td>\n",
       "      <td>75.31</td>\n",
       "      <td>-11.96</td>\n",
       "      <td>17.32</td>\n",
       "      <td>7.18</td>\n",
       "      <td>7.30</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>11.77</td>\n",
       "      <td>10.10</td>\n",
       "      <td>-14.10</td>\n",
       "      <td>13.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-507.40</td>\n",
       "      <td>62.42</td>\n",
       "      <td>-31.81</td>\n",
       "      <td>66.39</td>\n",
       "      <td>-4.85</td>\n",
       "      <td>27.72</td>\n",
       "      <td>6.52</td>\n",
       "      <td>12.40</td>\n",
       "      <td>5.83</td>\n",
       "      <td>6.58</td>\n",
       "      <td>6.15</td>\n",
       "      <td>-7.13</td>\n",
       "      <td>10.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-520.83</td>\n",
       "      <td>71.45</td>\n",
       "      <td>-14.09</td>\n",
       "      <td>62.63</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>22.61</td>\n",
       "      <td>3.19</td>\n",
       "      <td>12.69</td>\n",
       "      <td>2.68</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.67</td>\n",
       "      <td>12.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1       2       3      4      5      6      7      8      9   \\\n",
       "0  -525.07  76.78  -11.60   52.65 -11.82  23.17   3.46   9.32   4.81   9.84   \n",
       "1  -515.70  58.14  -28.15   60.27 -13.81  24.47  -0.66   9.37   6.75   6.76   \n",
       "2  -512.24  43.76  -42.25   60.95 -16.35  29.58   0.37   7.15   6.67   2.92   \n",
       "3  -506.90  46.93  -43.91   60.30 -17.97  27.71   2.81   9.62   8.36   7.00   \n",
       "4  -515.04  57.47  -34.75   58.05 -13.05  23.33   4.30  14.76   9.27   6.74   \n",
       "5  -518.08  59.62  -28.44   62.34  -5.90  28.66   4.44  12.78   5.71   3.10   \n",
       "6  -519.60  59.02  -29.47   58.88  -7.06  28.62   5.67  12.70   3.05  -1.19   \n",
       "7  -521.95  51.18  -39.10   50.69 -14.17  28.43   8.27  12.75   3.34   0.46   \n",
       "8  -520.59  49.49  -43.04   47.08 -17.83  26.43   6.32  13.32   7.48   6.81   \n",
       "9  -525.05  50.01  -40.06   49.56 -16.72  22.40   1.38   6.81   3.87   7.40   \n",
       "10 -526.81  46.46  -38.17   55.42 -12.63  22.49  -2.38   6.59   5.84   7.95   \n",
       "11 -523.35  53.00  -38.75   51.62 -10.86  23.17  -0.39  13.58  11.15   7.36   \n",
       "12 -521.51  57.29  -33.59   55.48  -7.68  28.77   4.25  14.94  13.17   8.71   \n",
       "13 -523.54  55.39  -30.58   59.78  -6.13  30.86   4.96  10.88   9.95  10.27   \n",
       "14 -525.72  51.37  -31.96   56.56  -9.50  33.38   5.11   7.44   6.55   8.82   \n",
       "15 -526.79  54.23  -27.46   57.25 -13.31  30.25   7.24  11.57   8.73   8.14   \n",
       "16 -526.16  52.92  -37.38   50.96  -9.34  27.12   4.61  15.23   7.53   5.47   \n",
       "17 -520.05  50.09  -40.47   56.38  -5.48  33.56   4.90   9.40   5.10   5.39   \n",
       "18 -514.36  59.53  -29.91   58.81  -7.00  34.88   4.92   8.42   5.23   6.00   \n",
       "19 -514.69  64.56  -22.60   61.95  -5.48  36.47   6.32   8.86   5.13   4.74   \n",
       "20 -517.48  62.91  -21.74   66.36  -5.58  32.10   4.74   7.53   7.89   8.64   \n",
       "21 -517.11  63.21  -22.48   67.24  -4.92  28.35   2.62   9.92  11.52  10.97   \n",
       "22 -520.01  57.55  -29.15   64.23  -5.26  28.22   3.83  11.41  12.17   8.56   \n",
       "23 -445.34  76.91  -63.10   66.46 -19.14  13.31  -1.92  -6.48  -1.73   8.79   \n",
       "24 -291.95  84.97 -123.77   95.94 -50.79  -9.73 -21.06 -21.25   0.05  12.16   \n",
       "25 -242.78  67.01 -148.01  110.22 -63.84 -15.82 -30.00 -18.67   3.44  12.76   \n",
       "26 -241.73  44.92 -162.19  121.68 -62.99 -20.57 -30.21  -6.43   8.98  10.25   \n",
       "27 -250.38  44.75 -159.20  134.61 -53.56 -20.17 -21.48   0.40   8.21   2.70   \n",
       "28 -265.88  42.97 -148.57  142.45 -53.35 -24.04 -13.38   0.59  -4.33  -5.44   \n",
       "29 -280.61  36.83 -139.14  147.86 -56.06 -24.66  -4.41   3.17 -15.92  -6.97   \n",
       "30 -300.41  25.31 -126.80  154.64 -52.92 -16.80   2.96   9.93 -20.84  -3.47   \n",
       "31 -313.69  21.77 -119.17  152.06 -53.26 -17.89   0.61  12.41 -25.33  -0.93   \n",
       "32 -306.70  34.26 -121.97  140.33 -48.99 -20.28  -6.39  11.23 -29.19  -3.38   \n",
       "33 -290.12  57.77 -128.74  123.46 -46.93 -22.48 -10.79  10.41 -19.91   0.62   \n",
       "34 -289.95  74.63 -130.34  105.17 -49.98 -23.01 -14.19   7.82  -7.19   4.14   \n",
       "35 -312.65  65.17 -127.94   95.80 -50.55 -14.60 -15.87   6.19   0.51  10.61   \n",
       "36 -344.59  61.19 -116.77   93.32 -44.63 -10.40 -12.44   6.72   8.73  19.25   \n",
       "37 -369.96  70.56 -106.67   84.97 -36.27 -13.31 -10.29   3.05   7.95  22.23   \n",
       "38 -390.19  71.79  -97.79   81.49 -30.05 -12.01 -10.34   1.56   6.70  22.31   \n",
       "39 -394.03  67.42  -87.92   91.77 -29.34  -6.93  -9.20   0.02   0.60  18.69   \n",
       "40 -425.23  64.37  -69.29   91.85 -25.13  -2.43  -4.07   1.22  -4.24  14.87   \n",
       "41 -492.60  59.90  -40.21   75.31 -11.96  17.32   7.18   7.30  -0.32  11.77   \n",
       "42 -507.40  62.42  -31.81   66.39  -4.85  27.72   6.52  12.40   5.83   6.58   \n",
       "43 -520.83  71.45  -14.09   62.63  -2.59  22.61   3.19  12.69   2.68   4.52   \n",
       "\n",
       "       10     11     12  \n",
       "0    9.49   0.30  11.87  \n",
       "1   10.54   0.10  13.83  \n",
       "2   14.63   6.03  13.35  \n",
       "3   16.61   4.43  10.99  \n",
       "4   11.68  -0.43  11.30  \n",
       "5    7.76  -2.59  10.42  \n",
       "6    2.74   1.17  12.24  \n",
       "7    6.95   4.85  12.14  \n",
       "8   12.11   7.06  14.94  \n",
       "9    9.82   0.11  12.36  \n",
       "10   8.60  -0.89  13.26  \n",
       "11   9.68   2.17  13.12  \n",
       "12  10.51   1.43  10.81  \n",
       "13   6.78  -2.27  13.67  \n",
       "14   5.45  -3.28  13.06  \n",
       "15   7.64  -1.69  13.27  \n",
       "16   8.19   0.01  12.24  \n",
       "17   6.59  -1.76  12.91  \n",
       "18   6.29  -4.30   6.21  \n",
       "19   5.61  -7.88   1.41  \n",
       "20   7.28  -7.72   3.15  \n",
       "21  10.49  -0.27   6.40  \n",
       "22   7.25   0.66   8.96  \n",
       "23  11.68  -0.37  11.11  \n",
       "24  13.14  -0.24  13.20  \n",
       "25  11.51   7.32  16.86  \n",
       "26  16.96  15.87  21.85  \n",
       "27  21.04  16.70  20.54  \n",
       "28  22.22  13.72  14.53  \n",
       "29  22.45   4.42   9.08  \n",
       "30  21.79  -4.00   9.96  \n",
       "31  20.35  -9.30  12.98  \n",
       "32  15.13  -6.05  19.31  \n",
       "33  14.35   2.71  27.48  \n",
       "34  13.95   5.42  34.19  \n",
       "35  15.95   1.14  37.18  \n",
       "36  18.55  -3.34  33.38  \n",
       "37  15.13  -2.87  32.09  \n",
       "38  11.74  -4.74  32.84  \n",
       "39  11.67 -12.71  33.27  \n",
       "40   8.40 -20.55  27.30  \n",
       "41  10.10 -14.10  13.79  \n",
       "42   6.15  -7.13  10.75  \n",
       "43   0.99   1.67  12.93  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , y = load_data ( input_data )\n",
    "# X.shape is 3D which is ( number_samples , les lignes , les colonnes ) \n",
    "matrice = X[1]\n",
    "matrice = np.round( matrice , 2 ) # just to avoid confusion when we view it \n",
    "\n",
    "df = pd.DataFrame(matrice )\n",
    "\n",
    "df    #so this  frame is a representation of MFcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69023a34-d440-4250-978b-3756d3728239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(data_path, test_size=0.2, validation_size=0.2):\n",
    "    X, y = load_data(data_path)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    # adding an axis to nd array  ( hadi wa9ila it's possible we can skip it [I'M NOT SURE] , it depends on model we building ) \n",
    "    X_train = X_train[..., np.newaxis] # CNN model needs an extra dimension for channel dimension\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83b988a2-db8d-4217-875f-eb9dd6d08ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sets loaded!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "544ac1bf-8ba6-4642-998b-32381cb72230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 44, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "print ( X_train.shape)\n",
    "# especially CNNs, where the input needs a channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b512782b-725b-458f-bb2b-43f4d150f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, loss=\"sparse_categorical_crossentropy\", learning_rate=0.0001):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(30, activation='softmax')) # 30 classes we can have \n",
    "\n",
    "    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss=loss,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7097bb4a-79e8-4ff8-8db7-4af7d1a031cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):\n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.001, patience=patience, restore_best_weights=True)\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_validation, y_validation),\n",
    "                        callbacks=[earlystop_callback])\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "307e8830-f637-492f-88e0-ceba66fa2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sets loaded!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21c34970-50d9-4773-ac34-02f53f98caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "print ( np.unique(y_train) )\n",
    "print ( np.unique(y_validation))\n",
    "print ( np.unique(y_test) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "30196126-4d68-4328-b299-2db973716b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,950</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │          \u001b[38;5;34m18,464\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m4,128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m10,304\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │           \u001b[38;5;34m1,950\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,998</span> (140.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,998\u001b[0m (140.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,742</span> (139.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,742\u001b[0m (139.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "model = build_model(input_shape, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "760974b7-530d-4840-bc74-ea47035d55af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.0216 - loss: 4.0885 - val_accuracy: 0.0082 - val_loss: 5.4546\n",
      "Epoch 2/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0348 - loss: 3.7150 - val_accuracy: 0.0082 - val_loss: 4.6760\n",
      "Epoch 3/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.0576 - loss: 3.5907 - val_accuracy: 0.0082 - val_loss: 4.3017\n",
      "Epoch 4/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.0621 - loss: 3.5476 - val_accuracy: 0.0082 - val_loss: 4.0481\n",
      "Epoch 5/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.0721 - loss: 3.4284 - val_accuracy: 0.0328 - val_loss: 3.8796\n",
      "Epoch 6/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.1154 - loss: 3.3114 - val_accuracy: 0.0492 - val_loss: 3.7605\n",
      "Epoch 7/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1060 - loss: 3.2593 - val_accuracy: 0.0574 - val_loss: 3.6483\n",
      "Epoch 8/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.1324 - loss: 3.1656 - val_accuracy: 0.0656 - val_loss: 3.5845\n",
      "Epoch 9/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.1418 - loss: 3.1366 - val_accuracy: 0.0902 - val_loss: 3.5388\n",
      "Epoch 10/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.1570 - loss: 3.0555 - val_accuracy: 0.0820 - val_loss: 3.5089\n",
      "Epoch 11/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.1939 - loss: 2.9719 - val_accuracy: 0.0738 - val_loss: 3.4723\n",
      "Epoch 12/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.2065 - loss: 2.9380 - val_accuracy: 0.0820 - val_loss: 3.4293\n",
      "Epoch 13/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.1845 - loss: 2.8964 - val_accuracy: 0.1066 - val_loss: 3.4113\n",
      "Epoch 14/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.2234 - loss: 2.8843 - val_accuracy: 0.1066 - val_loss: 3.4166\n",
      "Epoch 15/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.2606 - loss: 2.7946 - val_accuracy: 0.0984 - val_loss: 3.3799\n",
      "Epoch 16/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2586 - loss: 2.7749 - val_accuracy: 0.0984 - val_loss: 3.3478\n",
      "Epoch 17/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.2696 - loss: 2.6948 - val_accuracy: 0.0984 - val_loss: 3.3248\n",
      "Epoch 18/40\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.3125 - loss: 2.7057 - val_accuracy: 0.0984 - val_loss: 3.3001\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7796fa-076b-4c04-8693-7c65ad409ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
